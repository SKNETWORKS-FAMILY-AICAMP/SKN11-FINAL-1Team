import os
import re
import glob
import logging
from dotenv import load_dotenv
from qdrant_client import QdrantClient
from qdrant_client.models import VectorParams, Distance, PointStruct
from langchain_openai import OpenAIEmbeddings
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from loaders import load_documents
import uuid
from sklearn.feature_extraction.text import TfidfVectorizer
import os
from dotenv import load_dotenv

load_dotenv()

UPLOAD_BASE_DIR = os.getenv("UPLOAD_BASE_DIR", "uploaded_docs")
MEDIA_ROOT = os.getenv("MEDIA_ROOT", "media")

UPLOAD_BASE = os.path.abspath(os.path.join(MEDIA_ROOT, UPLOAD_BASE_DIR))

# ë¡œê¹… ì„¤ì •
os.makedirs("log", exist_ok=True)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("log/embedding.log", mode="a", encoding="utf-8")
    ]
)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë”© ë° ì„¤ì •
load_dotenv()
QDRANT_URL = os.getenv("QDRANT_URL")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
COLLECTION_NAME = "rag_multiformat"
VECTOR_SIZE = 3072

# Qdrant í´ë¼ì´ì–¸íŠ¸ ë° ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
client = QdrantClient(url=QDRANT_URL)
embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model="text-embedding-3-large")

# ì¡°í•­ íŒ¨í„´ ëª©ë¡ (ë©€í‹° í¬ë§· ëŒ€ì‘)
SECTION_PATTERNS = [
    r"(ì œ\s*\d+\s*ì¡°\s*[^\n]*)",  # ì œ1ì¡° ëª©ì 
    r"^\s*\d+\.\s*[^\n]+",        # 1. ê°œìš”
    r"^\s*\[\s*.+?\s*\]",         # [ëª©ì ]
    r"^\s*ì œ?\w+\s*ì¡°\s+[^\n]+"  # ì œì¼ì¡° ëª©ì 
]

# ì¡°í•­ ë‹¨ìœ„ë¡œ ì„¹ì…˜ ë¶„ë¦¬
def extract_sections_with_titles(text):
    for pattern in SECTION_PATTERNS:
        matches = list(re.finditer(pattern, text, re.MULTILINE))
        if len(matches) >= 3:  # ìµœì†Œ 3ê°œ ì´ìƒ ì¡°í•­ ê°ì§€ë˜ë©´ ì„±ê³µ
            logging.info(f"íŒ¨í„´ ë§¤ì¹˜ ì„±ê³µ: {pattern} / {len(matches)}ê°œ")
            sections = []
            for i, match in enumerate(matches):
                start = match.start()
                end = matches[i+1].start() if i+1 < len(matches) else len(text)
                title = match.group(1).strip() if match.groups() else match.group().strip()
                body = text[start + len(title):end].strip()
                sections.append((title, body))
            return sections
        else:
            logging.info(f"íŒ¨í„´ ë§¤ì¹˜ ì‹¤íŒ¨: {pattern}")
    
    logging.warning("ì¡°í•­ íŒ¨í„´ ë§¤ì¹˜ ì‹¤íŒ¨. ì „ì²´ ë¬¸ì„œë¥¼ ë‹¨ì¼ ì²­í¬ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")
    return [("ì „ì²´ë³¸ë¬¸", text)]

# Qdrantì— ì»¬ë ‰ì…˜ ìƒì„±
def create_collection_if_not_exists():
    collections = client.get_collections().collections
    if not any(c.name == COLLECTION_NAME for c in collections):
        client.create_collection(
            collection_name=COLLECTION_NAME,
            vectors_config=VectorParams(size=VECTOR_SIZE, distance=Distance.COSINE)
        )

def get_flexible_sections(text, fallback_chunk_size=700):
    # 1. ì¡°í•­ íŒ¨í„´ ì‹œë„
    sections = extract_sections_with_titles(text)
    if sections and len(sections) >= 3 and sections[0][0] != "ì „ì²´ë³¸ë¬¸":
        return sections

    logging.warning("âš  ì¡°í•­ íŒ¨í„´ ì‹¤íŒ¨ â†’ ë¬¸ë‹¨ ê¸°ë°˜ìœ¼ë¡œ ì¬ì‹œë„")

    # 2. ë¬¸ë‹¨ ê¸°ë°˜ ì‹œë„
    paragraphs = [p.strip() for p in text.split("\n\n") if len(p.strip()) > 50]
    if len(paragraphs) >= 3:
        return [(f"ë¬¸ë‹¨ {i+1}", para) for i, para in enumerate(paragraphs)]

    logging.warning("âš  ë¬¸ë‹¨ íŒ¨í„´ ì‹¤íŒ¨ â†’ ê³ ì • ê¸¸ì´ ì²­í¬ë¡œ ì¬ì‹œë„")

    # 3. ê³ ì • ê¸¸ì´ fallback
    chunks = [text[i:i+fallback_chunk_size] for i in range(0, len(text), fallback_chunk_size)]
    return [(f"ì²­í¬ {i+1}", chunk) for i, chunk in enumerate(chunks)]



# ê¸°ì¡´ì— ì €ì¥ëœ point ID ì¡°íšŒ
def get_existing_point_ids():
    create_collection_if_not_exists()
    existing_ids = set()
    scroll = client.scroll(collection_name=COLLECTION_NAME, with_payload=True, limit=10000)
    for point in scroll[0]:
        payload = point.payload or {}
        metadata = payload.get("metadata", {})
        file = metadata.get("source")
        chunk_id = metadata.get("chunk_id")
        if file is not None and chunk_id is not None:
            # file = os.path.abspath(file)
            file = os.path.normpath(os.path.abspath(file))
            existing_ids.add(f"{file}-{chunk_id}")
    return existing_ids





# ë¬¸ì„œ ì„ë² ë”© ë° Qdrant ì—…ë¡œë“œ (ê°œì„ ëœ ë²„ì „)
def advanced_embed_and_upsert(file_path, existing_ids, department_id=None, common_doc=False, original_file_name=None) -> int:
    """
    ê°œì„ ëœ ì„ë² ë”© ë° ì—…ë¡œë“œ í•¨ìˆ˜
    Args:
        file_path: íŒŒì¼ ê²½ë¡œ
        existing_ids: ê¸°ì¡´ point ID ì§‘í•©
        department_id: ë¶€ì„œ ID (í•„í„°ë§ìš©)
        common_doc: ê³µí†µ ë¬¸ì„œ ì—¬ë¶€
    Returns:
        ì—…ë¡œë“œëœ ì²­í¬ ìˆ˜
    """
    try:
        file_path = os.path.abspath(file_path)
        docs = load_documents(file_path)
        joined_text = "\n".join([doc.page_content for doc in docs])
        # sections = extract_sections_with_titles(joined_text)
        sections = get_flexible_sections(joined_text)

        
        logging.info(f"ë¬¸ì„œ ë¡œë“œ ê°œìˆ˜: {len(docs)}")
        logging.info(f"ì¡°í•­ ì¶”ì¶œ ê°œìˆ˜: {len(sections)}")
        
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=768,
            chunk_overlap=100,
            separators=["\n\n", "\n", ".", " ", ""],
        )
        
        split_docs = []
        for title, content in sections:
            chunks = text_splitter.split_text(content)
            for chunk in chunks:
                # combined_text = f"[{title}]\n{chunk}"
                combined_text = f"ì´ ë‚´ìš©ì€ '{title}'ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤.\n\n{chunk}"

                split_docs.append(Document(page_content=combined_text, metadata={"title": title}))
        
        logging.info(f"ìµœì¢… ì²­í¬ ê°œìˆ˜: {len(split_docs)}")
        
        texts = [doc.page_content for doc in split_docs]
        vectors = embeddings.embed_documents(texts)
        
        new_points = []
        for i, (doc, vector) in enumerate(zip(split_docs, vectors)):
            unique_key = f"{file_path}-{i}"
            if unique_key in existing_ids:
                continue
            
            # ë©”íƒ€ë°ì´í„° í™•ì¥ (ë¶€ì„œ ID, ê³µí†µ ë¬¸ì„œ ì—¬ë¶€, íŒŒì¼ëª… ì¶”ê°€)
            # doc.metadata["source"] = file_path
            # doc.metadata["source"] = os.path.normpath(os.path.abspath(file_path))
            # doc.metadata["source"] = os.path.relpath(file_path, start=UPLOAD_BASE).replace("\\", "/")
            doc.metadata["source"] = f"documents/{os.path.basename(file_path)}"


            doc.metadata["chunk_id"] = i
            # doc.metadata["department_id"] = department_id  # ë¶€ì„œ ID ì¶”ê°€
            doc.metadata["department_id"] = int(department_id) if department_id is not None else None
            doc.metadata["common_doc"] = common_doc        # ê³µí†µ ë¬¸ì„œ ì—¬ë¶€ ì¶”ê°€
            doc.metadata["file_name"] = os.path.basename(file_path)  # íŒŒì¼ëª… ì¶”ê°€
            doc.metadata["original_file_name"] = original_file_name  # ì›ë˜ ì—…ë¡œë“œëœ ì´ë¦„
            
            new_points.append(
    PointStruct(
        id=uuid.uuid4().int >> 64,  # âœ… ê³ ìœ í•œ ì •ìˆ˜ ID ìƒì„±
        vector=vector,
        payload={
            "text": doc.page_content,
            "metadata": doc.metadata
        }
    )
)
        
        if new_points:
            
            for point in new_points:
                logging.debug(
                    f"ğŸ“Œ ì—…ë¡œë“œ ì²­í¬: ID={point.id}, ë²¡í„° ê¸¸ì´={len(point.vector)}, ì œëª©={point.payload['metadata'].get('title')}"
                )

            client.upsert(collection_name=COLLECTION_NAME, points=new_points)
            logging.info(f"{file_path} â†’ ì‹ ê·œ ì²­í¬ {len(new_points)}ê°œ ì—…ë¡œë“œ ì™„ë£Œ")
            return len(new_points)
        else:
            logging.info(f"{file_path} â†’ ì´ë¯¸ ì €ì¥ëœ ì²­í¬ë§Œ ì¡´ì¬ (ì—…ë¡œë“œ ìƒëµ)")
            logging.info(f"{file_path} â†’ ì¤‘ë³µ ì²­í¬ {len(split_docs)}ê°œ, ì—…ë¡œë“œ ìƒëµë¨")
            return 0
            
    except Exception as e:
        logging.error(f"{file_path} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 0





# ë‹¤ì¤‘ íŒŒì¼ ìë™ ì²˜ë¦¬ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
if __name__ == "__main__":
    create_collection_if_not_exists()
    existing_ids = get_existing_point_ids()
    
    logging.info(f"existing_ids ê°œìˆ˜: {len(existing_ids)}")
    
    extensions = ["pdf", "docx", "csv", "txt", "md", "html", "pptx"]
    files = []
    for ext in extensions:
        files.extend(glob.glob(f"data/*.{ext}"))
    
    if not files:
        logging.warning("í´ë”ì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        total_chunks = 0
        total_files = 0
        for file_path in files:
            logging.info(f"\nì²˜ë¦¬ ì¤‘: {file_path}")
            added = advanced_embed_and_upsert(file_path, existing_ids)
            if added > 0:
                total_files += 1
                total_chunks += added
    
        # ìš”ì•½ ë³´ê³ 
        logging.info("\nì—…ë¡œë“œ ìš”ì•½")
        logging.info(f"ì‹ ê·œ ë¬¸ì„œ ìˆ˜: {total_files}")
        logging.info(f"ì´ ì‹ ê·œ ì²­í¬ ìˆ˜: {total_chunks}")
        logging.info("ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
