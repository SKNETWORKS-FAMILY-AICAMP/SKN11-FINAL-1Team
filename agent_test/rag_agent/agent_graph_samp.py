import os
import logging
import time
from dotenv import load_dotenv
from typing import TypedDict, List
from langgraph.graph import StateGraph, END
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from qdrant_client import QdrantClient

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("rag_graph_v2.log", mode="a", encoding="utf-8")
    ]
)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë”©
load_dotenv()
QDRANT_URL = os.getenv("QDRANT_URL")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
COLLECTION_NAME = "rag_multiformat"

# í´ë¼ì´ì–¸íŠ¸ ë° ëª¨ë¸
client = QdrantClient(url=QDRANT_URL, check_compatibility=False)
embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY, model="text-embedding-3-small")
llm = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model_name="gpt-4o-mini")

WINDOW_SIZE = 3

# State ì •ì˜
class AgentState(TypedDict, total=False):
    question: str
    contexts: List[str]
    answer: str
    reflection: str
    rewritten_question: str
    chat_history: List[str]
    rewrite_count: int

# RAG ì—¬ë¶€ íŒë‹¨
def decide_use_rag(state: AgentState) -> AgentState:
    return state

def get_use_rag_condition(state: AgentState) -> str:
    question = state["question"]
    prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì„ ì½ê³ , ì‚¬ë‚´ ë¬¸ì„œë‚˜ ê·œì •ê³¼ ê°™ì€ ì°¸ê³  ë¬¸ì„œê°€ í•„ìš”í•œ ì§ˆë¬¸ì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.

ì§ˆë¬¸: \"{question}\"

ë¬¸ì„œê°€ í•„ìš”í•˜ë©´ "use_rag", ì•„ë‹ˆë©´ "skip_rag"ë§Œ ì¶œë ¥í•˜ì„¸ìš”."""
    result = llm.invoke(prompt).content.strip().lower()
    return "use_rag" if "use" in result else "skip_rag"

# ë¬¸ì„œ ê²€ìƒ‰
def search_documents(state: AgentState) -> AgentState:
    query = state.get("rewritten_question") or state["question"]
    query_vec = embeddings.embed_query(query)
    results = client.search(
        collection_name=COLLECTION_NAME,
        query_vector=query_vec,
        limit=3,
        with_payload=True
    )
    contexts = []
    for r in results:
        title = r.payload.get("metadata", {}).get("title", "ë¬´ì œ")
        text = r.payload.get("text", "")
        contexts.append(f"[{title}]\n{text}")
    return {**state, "contexts": contexts}

# ë‹µë³€ ìƒì„±
def generate_answer(state: AgentState) -> AgentState:
    context = "\n---\n".join(state.get("contexts", []))
    question = state.get("rewritten_question") or state["question"]
    full_history = state.get("chat_history", [])
    recent_history = full_history[-WINDOW_SIZE:]
    history_text = "\n".join(recent_history)
    titles = [c.split('\n')[0].strip("[]") for c in state["contexts"] if c.startswith("[")]
    ref_titles = ", ".join(titles)
    fewshot = """ì˜ˆì‹œ :
Q: ì§€ê°ì´ 3íšŒ ëˆ„ì ë˜ë©´ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?
A: [ì œ12ì¡° ê·¼íƒœê´€ë¦¬ ê·œì •]ì— ë”°ë¥´ë©´ ì§€ê° 3íšŒëŠ” ê²°ê·¼ 1íšŒë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.

"""
    prompt = f"""{fewshot}
ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ê¸°ë¡:
{history_text}

ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ contextì— ì¶©ì‹¤í•˜ê²Œ ìì„¸íˆ ë‹µë³€í•˜ì„¸ìš”.

**ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ì„ ë”°ë¥´ì„¸ìš”**  
â†’ "ì œXì¡° ì¡°í•­ëª… ì— ë”°ë¥´ë©´ ..."  

ì°¸ê³  ì¡°í•­: {ref_titles}

Context:
{context}
Question: {question}
Answer:"""
    response = llm.invoke(prompt)
    updated_history = full_history + [f"Q: {question}\nA: {response.content}"]
    return {**state, "answer": response.content, "chat_history": updated_history}

# RAG ì—†ì´ ì§ì ‘ ë‹µë³€
def direct_answer(state: AgentState) -> AgentState:
    question = state["question"]
    full_history = state.get("chat_history", [])
    recent_history = full_history[-WINDOW_SIZE:]
    history_text = "\n".join(recent_history)
    fewshot = """ì˜ˆì‹œ :
Q: ì—°ì°¨ ì‹ ì²­ì€ ì–¸ì œê¹Œì§€ í•´ì•¼ í•˜ë‚˜ìš”?
A: [ê·¼íƒœê·œì •]ì— ë”°ë¥´ë©´ ì—°ì°¨ëŠ” ìµœì†Œ 3ì¼ ì „ê¹Œì§€ ì‹ ì²­í•´ì•¼ í•©ë‹ˆë‹¤.

"""
    prompt = f"""{fewshot}
ì§€ê¸ˆê¹Œì§€ì˜ ëŒ€í™” ë‚´ìš©:
{history_text}

Question: {question}
Answer:"""
    response = llm.invoke(prompt)
    updated_history = full_history + [f"Q: {question}\nA: {response.content}"]
    return {**state, "answer": response.content, "chat_history": updated_history}

# ë‹µë³€ í‰ê°€
def judge_answer(state: AgentState) -> AgentState:
    context = "\n---\n".join(state.get("contexts", []))
    question = state.get("rewritten_question") or state["question"]
    answer = state["answer"]
    prompt = f"""ë‹¤ìŒì€ ì§ˆë¬¸ê³¼ ë‹µë³€, ê·¸ë¦¬ê³  ì°¸ê³  contextì…ë‹ˆë‹¤.

Question: {question}
Context: {context}
Answer: {answer}

ë‹µë³€ì´ ì¶©ë¶„í•œê°€ìš”? ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”:

- í‰ê°€: [ì¶©ë¶„|ë¶€ì¡±]
- ì´ìœ : ..."""
    reflection = llm.invoke(prompt)
    return {**state, "reflection": reflection.content}

# ë¦¬í”Œë ‰ì…˜ ê²°ê³¼ íŒë‹¨ + ìµœëŒ€ ì¬ì‘ì„± ì œí•œ
def decide_to_reflect(state: AgentState) -> str:
    if state.get("rewrite_count", 0) >= 2:
        return "end"
    reflection = state.get("reflection", "")
    return "end" if any(kw in reflection for kw in ["ì¶©ë¶„", "ë¬¸ì œì—†ìŒ", "ì ì ˆ"]) else "rewrite"

# ì§ˆë¬¸ ì¬ì‘ì„±
def reformulate_question(state: AgentState) -> AgentState:
    question = state.get("rewritten_question") or state["question"]
    answer = state["answer"]
    context = "\n---\n".join(state.get("contexts", []))
    prompt = f"""ë‹µë³€ì´ ë¶€ì¡±í•˜ë‹¤ë©´, ì§ˆë¬¸ì„ ì¢€ ë” ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë°”ê¿” ì£¼ì„¸ìš”.

ê¸°ì¡´ ì§ˆë¬¸: {question}
ë‹µë³€: {answer}
Context: {context}

ë³´ê°• ì§ˆë¬¸:"""
    new_q = llm.invoke(prompt).content.strip()
    count = state.get("rewrite_count", 0) + 1
    return {**state, "rewritten_question": new_q, "rewrite_count": count}

# LangGraph êµ¬ì„±
builder = StateGraph(AgentState)
builder.add_node("decide", decide_use_rag)
builder.add_node("search", search_documents)
builder.add_node("answer", generate_answer)
builder.add_node("judge", judge_answer)
builder.add_node("rewrite", reformulate_question)
builder.add_node("direct_answer", direct_answer)

builder.set_entry_point("decide")

builder.add_conditional_edges("decide", get_use_rag_condition, {
    "use_rag": "search",
    "skip_rag": "direct_answer"
})
builder.add_edge("search", "answer")
builder.add_edge("answer", "judge")
builder.add_conditional_edges("judge", decide_to_reflect, {
    "end": END,
    "rewrite": "search"
})
builder.add_edge("rewrite", "search")
builder.add_edge("direct_answer", END)

graph = builder.compile()

# ì‹¤í–‰ ë£¨í”„
def run_agent():
    print("LangGraph ê¸°ë°˜ Agentic RAG ì‹œì‘ (ì¢…ë£Œ: exit)")
    state: AgentState = {"chat_history": [], "rewrite_count": 0}

    while True:
        question = input("\nì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")
        if question.strip().lower() == "exit":
            print("ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        state["question"] = question
        state.pop("rewritten_question", None)
        state["rewrite_count"] = 0  # ì§ˆë¬¸ë§ˆë‹¤ ì¹´ìš´í„° ì´ˆê¸°í™”

        start = time.time()
        final_state = graph.invoke(state)
        duration = time.time() - start
        logging.info(f"[run_agent] ì²˜ë¦¬ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ)")

        print(f"\nğŸ§  ìµœì¢… ë‹µë³€:\n{final_state.get('answer', '[ì‘ë‹µ ì—†ìŒ]')}")
        state["chat_history"] = final_state.get("chat_history", [])

if __name__ == "__main__":
    run_agent()
