# í‰ê°€ì ìˆ˜ê°€ ë°˜ì˜ëœ agent ì½”ë“œ
# 
# 
import sqlite3
import os
from dotenv import load_dotenv
from langchain.chat_models import ChatOpenAI

load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

llm = ChatOpenAI(
    temperature=0,
    model_name="gpt-4o",
    openai_api_key=openai_api_key
)

job = 'IT ê°œë°œì'

# í•˜ìœ„ subtasks ì¡°íšŒ
def fetch_subtasks_for_task(task_id: int):
    conn = sqlite3.connect("my_database2.db")
    cursor = conn.cursor()
    cursor.execute("""
        SELECT title, content
        FROM subtasks
        WHERE task_id = ?
    """, (task_id,))
    results = cursor.fetchall()
    conn.close()
    return results

# ìƒìœ„ task ì œëª© ì¡°íšŒ
def fetch_task_title(task_id: int):
    conn = sqlite3.connect("my_database2.db")
    cursor = conn.cursor()
    cursor.execute("SELECT title FROM tasks WHERE id = ?", (task_id,))
    result = cursor.fetchone()
    conn.close()
    return result[0] if result else "ì•Œ ìˆ˜ ì—†ëŠ” ì‘ì—…"

# í‰ê°€ ì§€í‘œë³„ í”„ë¡¬í”„íŠ¸ ì •ì˜
def get_score(prompt: str) -> float:
    response = llm.invoke(prompt)
    try:
        score = float(response.content.strip())
        return max(0.0, min(score, 1.0))  # Clamp between 0.0 and 1.0
    except ValueError:
        return -1.0  # Invalid score

# í‰ê°€ ë£¨í‹´
def evaluate_feedback(feedback: str, task_title: str, subtasks: list) -> dict:
    subtask_text = "\n".join([
        f"- {title.strip()}: {content.strip() if content else 'ë‚´ìš© ì—†ìŒ'}"
        for title, content in subtasks
    ]) or "ê´€ë ¨ í•˜ìœ„ ì‘ì—… ì—†ìŒ"

    prompts = {
        "answer_relevancy": f"""
        ì•„ë˜ í”¼ë“œë°±ì€ '{task_title}'ë¼ëŠ” ì£¼ì œì™€ í•˜ìœ„ ì‘ì—…ë“¤ì— ëŒ€í•´ ì˜ ê´€ë ¨ë˜ì–´ ìˆëŠ”ê°€?
        ì£¼ì œ ì í•©ë„(ê´€ë ¨ì„±)ë¥¼ 0.0ì—ì„œ 1.0 ì‚¬ì´ ì ìˆ˜ë¡œ í‰ê°€í•´ì¤˜.

        [ìƒìœ„ ì œëª©]: {task_title}
        [í•˜ìœ„ ì‘ì—…ë“¤]:
        {subtask_text}

        [í”¼ë“œë°±]:
        {feedback}

        ì ìˆ˜ë§Œ ìˆ«ìë¡œ ì¶œë ¥:
        """,

        "faithfulness": f"""
        ì•„ë˜ í”¼ë“œë°±ì€ ìƒìœ„ ì œëª© ë° í•˜ìœ„ ì‘ì—…ë“¤ì— ê¸°ë°˜í•˜ì—¬ ì‘ì„±ë˜ì—ˆëŠ”ê°€?
        ì—†ëŠ” ì‚¬ì‹¤ì„ ë§Œë“¤ê±°ë‚˜ ì¡°ì‘í•œ í‘œí˜„ ì—†ì´ ì¶©ì‹¤í•˜ê²Œ ì‘ì„±ë˜ì—ˆëŠ”ì§€ 0.0~1.0 ì ìˆ˜ë¡œ í‰ê°€í•´ì¤˜.

        [ìƒìœ„ ì œëª©]: {task_title}
        [í•˜ìœ„ ì‘ì—…ë“¤]:
        {subtask_text}

        [í”¼ë“œë°±]:
        {feedback}

        ì ìˆ˜ë§Œ ìˆ«ìë¡œ ì¶œë ¥:
        """,

        "answer_correctness": f"""
        ì•„ë˜ í”¼ë“œë°±ì€ ë¬¸ë²•, êµ¬ì„±, í‘œí˜„ë ¥ ë“± ì „ë°˜ì ìœ¼ë¡œ ë¦¬ë·°ë¡œì„œì˜ í’ˆì§ˆì´ ìš°ìˆ˜í•œê°€?
        ì‘ì„± í’ˆì§ˆì„ 0.0~1.0 ì ìˆ˜ë¡œ í‰ê°€í•´ì¤˜.

        [í”¼ë“œë°±]:
        {feedback}

        ì ìˆ˜ë§Œ ìˆ«ìë¡œ ì¶œë ¥:
        """
    }

    return {
        metric: get_score(prompt)
        for metric, prompt in prompts.items()
    }

# í”¼ë“œë°± ìƒì„± + í‰ê°€
def get_feedback_on_task(task_id: int):
    subtasks = fetch_subtasks_for_task(task_id)
    task_title = fetch_task_title(task_id)

    if not task_title:
        return "âŒ ìƒìœ„ íƒœìŠ¤í¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."

    if not subtasks:
        prompt = f"""
        ë„ˆëŠ” ì‚¬ë‚´ ì˜¨ë³´ë”© ê³¼ì œë¥¼ í‰ê°€í•˜ëŠ” ë©˜í† ì•¼. íŒë‹¨í•  ì—…ë¬´ ì œëª©ì€ '{task_title}'ì•¼.
        ì œëª©ì„ ê¸°ë°˜ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì „ì²´ì ì¸ í”¼ë“œë°±ì„ ì „ë¬¸ê°€ì²˜ëŸ¼ ì‘ì„±í•´ì¤˜
                
        ê° í•˜ìœ„ ì‘ì—…ì„ ì°¸ê³ í•´ì„œ ì•„ë˜ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì „ì²´ì ì¸ í”¼ë“œë°±ì„ ì „ë¬¸ê°€ì²˜ëŸ¼ ì‘ì„±í•´ì¤˜
        ì½”ë“œê°€ ìˆì„ ì‹œ, ì½”ë“œ ë¦¬ë·°ë„ í¬í•¨í•´ì„œ ì‘ì„±í•´ì¤˜ 


        - ğŸ‘ ì˜ëœ ì :
        - ğŸ”§ ê°œì„ í•  ì :
        - ğŸ§¾ ìš”ì•½ í”¼ë“œë°±: 
        """
    else:
        subtask_descriptions = "\n".join([
            f"- {title.strip()}: {content.strip() if content else 'ë‚´ìš© ì—†ìŒ'}"
            for title, content in subtasks
        ])

        prompt = f"""
        ë„ˆëŠ” ì‚¬ë‚´ ì˜¨ë³´ë”© ê³¼ì œë¥¼ í‰ê°€í•˜ëŠ” ë©˜í† ì•¼. íŒë‹¨í•  ì—…ë¬´ ì œëª©ì€ '{task_title}'ì•¼.
        ì œëª©ì„ ê¸°ë°˜ìœ¼ë¡œ ì•„ë˜ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì „ì²´ì ì¸ í”¼ë“œë°±ì„ ì „ë¬¸ê°€ì²˜ëŸ¼ ì‘ì„±í•´ì¤˜
                
        ê° í•˜ìœ„ ì‘ì—…ì„ ì°¸ê³ í•´ì„œ ì•„ë˜ì™€ ê°™ì€ í˜•ì‹ìœ¼ë¡œ ì „ì²´ì ì¸ í”¼ë“œë°±ì„ ì „ë¬¸ê°€ì²˜ëŸ¼ ì‘ì„±í•´ì¤˜
        ì½”ë“œê°€ ìˆì„ ì‹œ, ì½”ë“œ ë¦¬ë·°ë„ í¬í•¨í•´ì„œ ì‘ì„±í•´ì¤˜ 


        - ğŸ‘ ì˜ëœ ì :
        - ğŸ”§ ê°œì„ í•  ì :
        - ğŸ§¾ ìš”ì•½ í”¼ë“œë°±: 
        """

    feedback = llm.invoke(prompt).content
    scores = evaluate_feedback(feedback, task_title, subtasks)

    return feedback, scores


if __name__ == "__main__":
    task_id = 1
    feedback, scores = get_feedback_on_task(task_id)

    print("ğŸ“‹ í”¼ë“œë°±:\n")
    print(feedback)

    print("\nğŸ“Š ìë™ í‰ê°€ ì ìˆ˜:")
    for metric, score in scores.items():
        if score == -1.0:
            print(f"âŒ {metric}: í‰ê°€ ì‹¤íŒ¨ (LLM ì‘ë‹µ ì˜¤ë¥˜)")
        else:
            print(f"{metric}: {score:.2f}")
